{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d51ba5",
   "metadata": {},
   "source": [
    "Hi, you need to install: pandas, requests, BeautifulSoup, re, datetime, pytube, threadpoolctl, numpy, pymongo, os.\n",
    "\n",
    "Web scraping return a document call Nasa_apod.csv from the API NASA APOD, I extract the information: date, name, link, format(\"imagen\" or \"other\"for video, gif), Explanation and download of each picture of the nasa.\n",
    "We send a little example because is a big file.  \n",
    "\n",
    "Store data in MongoDB, we cann't do it. Sorry so much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71df11ca-90fe-4430-881c-602d23a0b375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " web_scraping_pythonic_force.ipynb\n",
      "'Zebrafish Embryo Mask Generation from Bright Field Images Result.ipynb'\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b0fbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the next directory, we save the images\n",
      "/home/user/Escritorio/Thesis/Dorothy/web_scraping/imagen_pythonic\n",
      "\n",
      " Start time  2023-12-10 23:48:11.295651\n",
      "You have 4  cores\n",
      "---- Instanciar\n",
      "-----Ejecutar\n",
      "-----Espera\n",
      "Video at 2023 December 03\n",
      "------- Regreso a la ejercicion inicial\n",
      "\n",
      " End time  2023-12-10 23:48:41.266846\n",
      "\n",
      " Lag Time 0:00:29.971195\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "from pytube import YouTube\n",
    "from threading import Thread\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "!mkdir imagen_pythonic #CREATE A FOLDER CALLED \"imagen_pythonic\" WITHIN THE DIRECTORY \n",
    "os.chdir(\"imagen_pythonic\") #CHANGE THE DIRECTORY \n",
    "print(\"In the next directory, we save the images\")\n",
    "!pwd\n",
    "url = 'https://apod.nasa.gov/apod/archivepix.html' #URL FOR WEB SCRAPING\n",
    "\n",
    "response = requests.get(url) # Make the GET request\n",
    "soup = BeautifulSoup(response.content, 'html.parser') # Parse HTML content\n",
    "entriesa= soup.find_all('a') # FIND ALL ENTRANCE WITH  LABEL \"a\" \n",
    "andya=entriesa[3:-11]  ##DELETE UPPER AND LOWER TEXT    \n",
    "ti = datetime.datetime.now() #GET EXACTLY TIME OF THE START\n",
    "print(\"\\n Start time \", ti)\n",
    "data=[] #CREATE A VECTOR TO SAVE THE CONTENT\n",
    "\n",
    "#########________FUNCTION: EXTRACT EACH TITLE, LINK, DATE, LINK IN EVERY PAGE WEB OF THE APOD-API NASA\n",
    "def funcion(andya, xi,xf ): \n",
    "    for k in range(xi,xf):\n",
    "        date = andya[k].previous_sibling.strip()  # GET THE DATE\n",
    "        date=date.replace(\":\",\"\")\n",
    "        title = andya[k].text.strip()  # GET THE TITLE\n",
    "        link = andya[k-1].find_next('a')['href']  # GET THE LINK\n",
    "#         print(f'k={k},Date: {date}, Title: {title}, Link: https://apod.nasa.gov/apod/{link}')\n",
    "\n",
    "        ########Extraction of the text in each link\n",
    "        url = f\"https://apod.nasa.gov/apod/{link}\"\n",
    "        response = requests.get(url)   \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        entries=soup.find_all('p')\n",
    "        entries= BeautifulSoup(str(entries[2]), 'html.parser')\n",
    "        texto_sin_html = entries.get_text(strip=False)\n",
    "        texto_sin_html=texto_sin_html[16:]\n",
    "        texto_sin_html=texto_sin_html.replace(\"\\n\",\"\")\n",
    "\n",
    "        test_str = texto_sin_html\n",
    "        wrd =\"Tomorrow's\"\n",
    "        test_str = test_str.split()\n",
    "        res = -1\n",
    "        s=0\n",
    "        for idx in test_str: \n",
    "            if len(re.findall(wrd, idx)) > 0:\n",
    "                res = test_str.index(idx) + 1\n",
    "                break\n",
    "            s+=len(idx)\n",
    "        texto_sin_html=texto_sin_html[:s+res]\n",
    "        ############################# End of the extraction of the text\n",
    "        \n",
    "        ######################## Download imagen of each link\n",
    "        #FIND LABEL <img> AND GET  ATTRIBUTE \"src\"  AND EXTRACT  IMAGE'S URL\n",
    "        entries=soup.find_all(\"img\")\n",
    "        if not entries:\n",
    "            print(\"Video at\",date)\n",
    "            data.append([date, title,f\"https://apod.nasa.gov/apod/{link}\",\"Other\",texto_sin_html] )\n",
    "            continue\n",
    "            \n",
    "        data.append([date, title,f\"https://apod.nasa.gov/apod/{link}\",\"Image\",texto_sin_html] )\n",
    "        image_url = soup.find('img')['src']\n",
    "        image_url=f'https://apod.nasa.gov/apod/{image_url}'\n",
    "        # Section: download imagen\n",
    "        response = requests.get(image_url)\n",
    "        \n",
    "        # Write the image in a file \n",
    "        d1=date.replace(\" \",\"_\")\n",
    "        with open(f'{d1}.jpg', 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        ################### End download of each link    \n",
    "    \n",
    "    \n",
    "############____________________ END FUNCTION    \n",
    "############################### MULTITHREADING  \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    hilos=[]\n",
    "    cores=os.cpu_count()\n",
    "    print(\"You have\",cores,\" cores\")\n",
    "    ###################################\n",
    "    x0=int(len(andya)/cores)\n",
    "    xi=1\n",
    "    xf=x0+1\n",
    "    ####################################\n",
    "    print(\"---- Instanciar\")\n",
    "    \n",
    "    for n in range (cores-1):\n",
    "        hilo=Thread(target=funcion, args=(andya, xi,xf )) ##input function for n-1 cores\n",
    "        hilos.append(hilo)\n",
    "        xi+=x0\n",
    "        xf+=x0\n",
    "    xf=len(andya)    \n",
    "    hilo=Thread(target=funcion, args=(andya, xi,xf )) ##nput function for the last core\n",
    "    hilos.append(hilo)\n",
    "    print(\"-----Ejecutar\")\n",
    "    \n",
    "    for hilo in hilos:\n",
    "        hilo.start()    \n",
    "    print(\"-----Espera\")\n",
    "    \n",
    "    for hilo in hilos:\n",
    "        hilo.join()\n",
    "    print(\"------- Regreso a la ejercicion inicial\")\n",
    "    \n",
    "    #########________Merge the information\n",
    "    data=pd.DataFrame(data)\n",
    "    data=data.rename(columns={0:\"Date\",1:\"Name\",2:\"Link\",3:\"Format\",4:\"Explanation\"})\n",
    "\n",
    "    tf = datetime.datetime.now()\n",
    "    print(\"\\n End time \", tf)\n",
    "\n",
    "    print(\"\\n Lag Time\",tf-ti)\n",
    "    ########________End the MULTITHREADING  \n",
    "    \n",
    "    \n",
    "year=[]\n",
    "day=[]\n",
    "month=[]\n",
    "for k in data[\"Date\"]:\n",
    "    year.append(k[:4])\n",
    "    day.append(k[-2:])\n",
    "    month.append(k[5:-3])\n",
    "    \n",
    "data[\"day\"]=pd.DataFrame(day)\n",
    "data[\"month\"]=pd.DataFrame(month)\n",
    "data[\"year\"]=pd.DataFrame(year)\n",
    "del data[\"Date\"]\n",
    "new_cols = [\"day\",\"month\",\"year\",\"Name\",\"Link\",\"Format\",\"Explanation\"]\n",
    "data = data[new_cols]\n",
    "data.to_csv('Nasa_apod.csv',index=False) #Save all information in file .csv, its name is \"Nasa_apod.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "MONGO_HOST=\"localhost\" #\"mongodb://localhost\",27017)\n",
    "MONGO_PUERTO=\"27017\"\n",
    "MONGO_TIEMPO_FUERA=1000\n",
    "\n",
    "MONGO_URI=\"mongodb://\"+MONGO_HOST+\":\"+MONGO_PUERTO+\"/\"\n",
    "\n",
    "Mongo_basedatos=\"Nasa\"\n",
    "Mongo_coleccion=\"Picture\"\n",
    "\n",
    "\n",
    "try:\n",
    "    cliente=pymongo.MongoClient(MONGO_URI,serverSelectionTimeoutMS=MONGO_TIEMPO_FUERA)\n",
    "    baseDatos=[Mongo_basedatos]\n",
    "    coleccion=baseDatos[Mongo_coleccion]\n",
    "    for documento in coleccion.find():\n",
    "        print(documento)\n",
    "    \n",
    "#     cliente.server_info()\n",
    "#     print(\"Conexion a mongo exitosa\")\n",
    "    cliente.close()\n",
    "except pymongo.errors.ServerSelectionTimeoutError as errorTiempo:\n",
    "    print(\"Tiempo excedido\"+errorTiempo)\n",
    "except pymongo.errors.ConnectionFailure as errorConexion:\n",
    "    print(\"Fallo al conectarse a mongodb\"+errorConexion)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
